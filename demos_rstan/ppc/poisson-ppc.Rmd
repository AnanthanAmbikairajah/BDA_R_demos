---
title: "Detecting model misfit with posterior predictive checks"
author: "Jonah Gabry, Aki Vehtari"
date: "Last modified `r format(Sys.Date())`."
output:
  html_document:
    fig_caption: yes
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
    theme: readable
    code_download: true
---

# Setup  {.unnumbered}

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE, echo = TRUE, comment=NA, fig.align="center")
```

**Load packages**
```{r load-packages, message=FALSE, warning=FALSE}
library(rstan)
library(ggplot2)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(loo)
library(rprojroot)
root<-has_file(".BDA_R_demos_root")$make_fix_file()
```


# Load and examine data
```{r poisson-data, fig.width = 4, fig.height = 4}
source(root("demos_rstan/ppc", "count-data.R"))
print(N)
print(y)
qplot(y)
```

## Plot histogram of draws from Poisson with same mean

```{r plot-x, fig.width = 4, fig.height = 4}
x <- rpois(N, mean(y))
qplot(x)
```

```{r plot-y-x, message=FALSE}
plotdata <- data.frame(
  value = c(y, x), 
  variable = rep(c("Our data", "Poisson data"), each = N)
)
ggplot(plotdata, aes(x = value, color = variable)) + 
  geom_freqpoly(binwidth = 0.5) +
  scale_x_continuous(name = "", breaks = 0:max(x,y)) +
  scale_color_manual(name = "", values = c("gray30", "purple"))
```

# Fit basic Poisson model

Even though we already suspect it won't be a good model for this data, it's
still a good idea to start by fitting the simplest Poisson model. From there we
can then identify in which ways the model is inadequate.
```{r}
code_poisson <- root("demos_rstan/ppc", "poisson-simple.stan")
writeLines(readLines(code_poisson))
```


```{r, fit}
fit <- stan(file = code_poisson, data = c("y", "N"), refresh = 0)
monitor(as.array(fit)[,,c("lambda","lp__")])
```

## Look at posterior distribution of lambda

```{r, plot-lambda}
color_scheme_set("brightblue") # check out ?bayesplot::color_scheme_set
lambda_draws <- as.matrix(fit, pars = "lambda")
mcmc_areas(lambda_draws, prob = 0.8) # color 80% interval
```

## Check summary of lambda and compare to mean of the data
```{r, print-fit}
print(fit, pars = "lambda")
mean(y)
```
The model gets the mean right, but, as we'll see next, the model is quite bad
at predicting the outcome.

# Graphical posterior predictive checks

## Extract `y_rep` draws from the fitted model object

```{r y_rep}
y_rep <- as.matrix(fit, pars = "y_rep")

# number of rows = number of post-warmup posterior draws
# number of columns = length(y)
dim(y_rep) 
```

## Compare histogram of `y` to histograms of several `y_rep`s

```{r ppc-hist, message=FALSE}
ppc_hist(y, y_rep[1:8, ], binwidth = 1)
```

## Compare density estimate of `y` to density estimates of a bunch of `y_rep`s

```{r ppc-dens-overlay}
ppc_dens_overlay(y, y_rep[1:50, ])
```

## Rootogram shows the square root of counts

Rootogram shows the square root of counts, with histogram for the observed data and and model predictions (yrep) as a ribbon

```{r ppc-rootogram}
ppc_rootogram(y, y_rep)
```

## Compare proportion of zeros in `y` to the distribution of that proportion over all `y_rep`s

```{r prop-zero, message=FALSE}
prop_zero <- function(x) mean(x == 0)
print(prop_zero(y))

ppc_stat(y, y_rep, stat = "prop_zero")
```

## Some other checks

Look at two statistics in a scatterplot:

```{r stat-2d}
ppc_stat_2d(y, y_rep, stat = c("mean", "sd"))
```

Distributions of predictive errors:

```{r, predictive-errors}
ppc_error_hist(y, y_rep[1:4, ], binwidth = 1) + xlim(-15, 15)
```

# Fit Poisson "hurdle" model (also with truncation from above)

This model says that there is some probability `theta` that `y`
is zero and probability `1 - theta` that `y` is positive. 
Conditional on observing a positive `y`, we use a truncated 
Poisson
```
y[n] ~ Poisson(lambda) T[1, U];
```
where `T[1,U]` indicates truncation with lower bound `1` and upper bound `U`, 
which for simplicity we'll _assume_ is `max(y)`.

```{r}
code_poisson_hurdle <- root("demos_rstan/ppc", "poisson-hurdle.stan")
writeLines(readLines(code_poisson_hurdle))
```


```{r fit-2}
fit2 <- stan(file = code_poisson_hurdle, data = c("y", "N"), refresh = 0)
```

```{r, print-fit2}
print(fit2, pars = c("lambda", "theta"))
```

## Compare posterior distributions of lambda from the two models
```{r, compare-lambdas}
lambda_draws2 <- as.matrix(fit2, pars = "lambda")
lambdas <- cbind(lambda_fit1 = lambda_draws[, 1],
                 lambda_fit2 = lambda_draws2[, 1])
mcmc_areas(lambdas, prob = 0.8) # color 80% interval
```

# Posterior predictive checks again

Same plots as before (and a few others), but this time using `y_rep` from `fit2`.
Everything looks much more reasonable:

```{r ppc-hist-2, message=FALSE}
y_rep2 <- as.matrix(fit2, pars = "y_rep")
ppc_hist(y, y_rep2[1:8, ], binwidth = 1)
```

```{r ppc-dens-overlay-2}
ppc_dens_overlay(y, y_rep2[1:50, ])
```

```{r ppc-rootogram-2}
ppc_rootogram(y, y_rep2)
```

```{r, prop-zero-2, message=FALSE}
ppc_stat(y, y_rep2, stat = "prop_zero")
```


```{r, more-checks, message=FALSE}
ppc_stat_2d(y, y_rep2, stat = c("mean", "sd"))
ppc_error_hist(y, y_rep2[sample(nrow(y_rep2), 4), ], binwidth = 1)
```

# Comparison with leave-one-out cross-validation

Compare predictive performance with LOO.

```{r}
(loo1 <- loo(fit))
(loo2 <- loo(fit2))
loo_compare(loo1, loo2)
```

Model 2 is a clear winner in the predictive performance.

